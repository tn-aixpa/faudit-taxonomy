{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd79a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from datetime import datetime\n",
    "from umap import UMAP\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bcc45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SEZIONE 1: Preparazione dati e parametri\n",
    "# -----------------------------------------------------------------------------\n",
    "# Questa sezione del notebook:\n",
    "# 1) Imposta i parametri di input per l'analisi (tipo di dati testuali da utilizzare (nell'esempio Titolo e descrizione) e numero di categorie (nell'esempio 100 categorie)).\n",
    "# 2) Carica il dataset delle azioni da CSV.\n",
    "# 3) Crea una cartella di output con timestamp per salvare i risultati in modo univoco.\n",
    "# 4) Filtra le righe con il campo 'descrizione' non vuoto.\n",
    "# 5) Costruisce una nuova colonna testuale 'text' combinando 'titolo' e 'descrizione'.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# 1)--- Inputs ---\n",
    "TYPE_OF_ANALYSIS = \"TopicModelling_TitoloDescrizione_\"  # in questa analisi usiamo sia titolo che descrizione. \n",
    "                                          # È possibile aggiungere altri campi, ad es. \"obiettivi\".\n",
    "N_TOPICS = 100  # numero di topic/categorie che il modello deve trovare; regolarlo in base alla granularità desiderata.\n",
    "\n",
    "# 2) --- Caricamento dati ---\n",
    "df = pd.read_csv(\"./FT/azioni.csv\") # aggiustare il percorso del file CSV se necessario\n",
    "\n",
    "# 3) \n",
    "# --- Timestamp per cartella risultati (es. 20251030-1423) ---\n",
    "stamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "# --- Nome cartella ---\n",
    "folder_name = f\"{stamp}{TYPE_OF_ANALYSIS}n{N_TOPICS}\"\n",
    "\n",
    "# --- Creazione directory dei risultati ---\n",
    "\n",
    "mydir= './'+TYPE_OF_ANALYSIS+stamp + '_'+ str(N_TOPICS)+'topics/' # nel github per comodità la cartella è chiamata semplicemente \"Results\"\n",
    "\n",
    "\n",
    "os.mkdir(mydir)\n",
    "print(\"Created:\", mydir)\n",
    "\n",
    "# 4) --- Filtro e costruzione ---\n",
    "# Mantiene solo le righe con 'descrizione' non nulla\n",
    "df_filtered = df.dropna(subset=['descrizione']).copy()\n",
    "\n",
    "# 5) --- Filtro e costruzione ---\n",
    "# Crea la colonna 'text' combinando 'titolo' e 'descrizione' (robusto a NaN/whitespace)\n",
    "df_filtered['text'] = (\n",
    "    'Titolo: '\n",
    "    + df_filtered['titolo'].fillna('').astype(str).str.strip()\n",
    "    + '; Descrizione: '\n",
    "    + df_filtered['descrizione'].astype(str).str.strip()\n",
    ")\n",
    "\n",
    "print(\"Totale documenti (titolo azione + descrizione) utilizzati per l'analisi:\", len(df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5091323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SEZIONE 2: Pulizia del testo (italiano) – mantiene la punteggiatura\n",
    "# -----------------------------------------------------------------------------\n",
    "# Questa sezione del notebook:\n",
    "# - Tokenizza il testo con NLTK `word_tokenize` (mantiene la punteggiatura).\n",
    "# - Rimuove le stopword italiane di NLTK (confronto case-insensitive).\n",
    "# - Restituisce una lista di stringhe pulite da usare, ad esempio, con BERTopic.\n",
    "# Nota: non forza il lowercase dell’output e non rimuove la punteggiatura.\n",
    "# =============================================================================\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('italian'))\n",
    "\n",
    "def drop_stopwords(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return \" \".join(w for w in tokens if w.lower() not in STOP_WORDS)\n",
    "\n",
    "# Applica alla colonna e ottiene una lista\n",
    "docs_unique = df_filtered['text'].fillna('').map(drop_stopwords).tolist()\n",
    "\n",
    "print(docs_unique[10])  # esempio di documento pulito\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SEZIONE 3: creazione del modello con BERTopic e riduzione dimensionale al numero di categorie desiderate\n",
    "# -----------------------------------------------------------------------------\n",
    "# Questa sezione del notebook:\n",
    "# - Configura UMAP per ridurre la dimensionalità delle embedding testuali.\n",
    "# - Inizializza BERTopic per lingua italiana con i parametri dell’analisi.\n",
    "# - Addestra il modello sui documenti e restituisce l’assegnazione dei topic.\n",
    "# - Estrae gli embedding dei documenti dal modello addestrato e li riduce a 2 dimensioni per la visualizzazione (es. scatter).\n",
    "# =============================================================================\n",
    "\n",
    "# UMAP per la riduzione dimensionale interna usata da BERTopic\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,      # equilibrio tra struttura locale/globale\n",
    "    n_components=5,      # dimensione target per la clusterizzazione\n",
    "    min_dist=0.0,        # addensa i punti; utile per separare i cluster\n",
    "    metric='cosine',     # adatta a embedding testuali\n",
    "    random_state=42      # riproducibilità\n",
    ")\n",
    "\n",
    "# Inizializza BERTopic con parametri principali dell'analisi\n",
    "topic_model = BERTopic(\n",
    "    language=\"multilingual\",          # italian is not supported    \n",
    "    calculate_probabilities=False,   # più veloce, meno memoria\n",
    "    verbose=True,                    \n",
    "    nr_topics=N_TOPICS,              # numero di topic desiderato\n",
    "    top_n_words=30,                  # n, parole rappresentative mostrate per topic\n",
    "    umap_model=umap_model            # UMAP definito sopra per ri\n",
    ")\n",
    "\n",
    "# Addestra il modello e ottiene:\n",
    "# - topics: indice di topic per ciascun documento\n",
    "topics, prob = topic_model.fit_transform(docs_unique)\n",
    "\n",
    "# Estrae gli embedding dei documenti e riduzione a 2D per plotting/visualizzazione\n",
    "embeddings = topic_model._extract_embeddings(docs_unique, method=\"document\")\n",
    "reduced_embeddings = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=2,\n",
    "    min_dist=0.0,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ").fit_transform(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SEZIONE 4: Salvataggio del modello e dei risultati (BERTopic)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Questa sezione del notebook:\n",
    "# - Crea la cartella del modello e salva il modello BERTopic (CTFIDF + embedding model).\n",
    "# - Esporta l’assegnazione documento→topic.\n",
    "# - Esporta le informazioni riassuntive dei topic (descrizioni, conteggi, ecc.).\n",
    "# - Unisce i metadati originali dei documenti con le info dei topic e salva i risultati.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# --- Cartella modello ---\n",
    "\n",
    "model_dir = mydir +\"BERTopicModel\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "# Modello di embedding usato, in questo caso è il default (deve corrispondere a quello del training)\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# --- Salvataggio modello BERTopic ---\n",
    "# serialization=\"pytorch\": salva i pesi in formato torch\n",
    "# save_ctfidf=True: salva la matrice C-TF-IDF (utile per ricaricare topic/termini)\n",
    "# save_embedding_model=...: salva anche il modello di embedding\n",
    "topic_model.save(\n",
    "    str(model_dir),\n",
    "    serialization=\"pytorch\",\n",
    "    save_ctfidf=True,\n",
    "    save_embedding_model=embedding_model\n",
    ")\n",
    "\n",
    "# --- Esporta informazioni sui topic ---\n",
    "# get_topic_info() restituisce, tra le altre, le colonne: Topic, Count, Name\n",
    "\n",
    "topic_info = topic_model.get_topic_info()\n",
    "del topic_info['Representative_Docs']  # rimuove colonna non necessari per avere un ouput più leggero\n",
    "topic_info= topic_info.drop(df.index[0]) #la prima riga si riferisce a topic -1 (outliers non assegnati a nessuna categoria), non di interesse per successive analisi \n",
    "topic_info.to_csv(mydir +\"topics_overview.tsv\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76390ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SEZIONE 5: Gerarchia dei topic (dendrogramma).\n",
    "# Ispezione visiva delle relazioni tra i topic e i nomi dei topic. Utile anche per effettuare un sanity check dell'analisi\n",
    "# -----------------------------------------------------------------------------\n",
    "# Questa sezione:\n",
    "# - Genera il dendrogramma gerarchico dei topic individuati da BERTopic.\n",
    "# - Salva la stessa visualizzazione in un file HTML interattivo.\n",
    "# - Salva la struttura del dendrogramma in un file di testo.\n",
    "# =============================================================================\n",
    "\n",
    "# Mostra il dendrogramma nel notebook \n",
    "hierarchical_topics = topic_model.hierarchical_topics(docs_unique)\n",
    "hierarchical_topics.to_csv(mydir + \"hierarchical_topics.csv\")\n",
    "\n",
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "f_out = open(mydir+'/topics_tree.txt','w')\n",
    "f_out.write(tree)\n",
    "\n",
    "# Crea la figura del dendrogramma (identica a quella mostrata sopra)\n",
    "fig1 = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "\n",
    "# Salva la figura come HTML interattivo nella cartella dei risultati.\n",
    "fig1.write_html(mydir + \"/Dendrogram_taxonomy.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_aixpa (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
